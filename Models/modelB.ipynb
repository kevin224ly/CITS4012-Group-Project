{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11585724",
   "metadata": {},
   "source": [
    "# Model B – ESIM-Style BiGRU with Inference Composition\n",
    "\n",
    "This notebook implements the ESIM-style BiGRU model (Model B) for the CITS4012 Natural Language Processing project. The implementation adheres to the project specification by using the provided science-domain NLI dataset, training all parameters from scratch with PyTorch, and avoiding any prohibited resources (e.g., Hugging Face models). Detailed explanations accompany each step to keep the workflow transparent and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2e8da",
   "metadata": {},
   "source": [
    "## Specification Alignment\n",
    "- Inputs come exclusively from `train.json`, `validation.json`, and `test.json` supplied with the project.\n",
    "- Only PyTorch (permitted framework) is used for modelling; no pretrained language model checkpoints are loaded.\n",
    "- A shared BiGRU encoder with soft-alignment attention, local inference composition, and pooling satisfies the architectural requirements for Model B.\n",
    "- The notebook records configuration, training, and evaluation logs so the reported numbers are reproducible and compliant with the requirement to include the running log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fd6b5",
   "metadata": {},
   "source": [
    "## Implementation Roadmap\n",
    "1. Configure deterministic training utilities and parse the dataset according to the specification.\n",
    "2. Inspect the dataset to understand label balance and sequence-length statistics for science-domain premises and hypotheses.\n",
    "3. Build a vocabulary from the training split, implement token-to-index conversion, and construct PyTorch datasets/dataloaders with dynamic padding.\n",
    "4. Define the ESIM-style BiGRU architecture, covering encoding, cross-attention, local inference enhancement, composition, and classification.\n",
    "5. Train the model with early stopping on the validation set, evaluate on validation and test data, and report detailed metrics.\n",
    "6. Run a brief qualitative attention inspection to ground the alignment behaviour of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebea65a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:26.976727Z",
     "iopub.status.busy": "2025-10-15T09:34:26.976458Z",
     "iopub.status.idle": "2025-10-15T09:34:28.419586Z",
     "shell.execute_reply": "2025-10-15T09:34:28.419295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/Desktop/CITS4012/Group Project/CITS4012-Group-Project/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17017294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.420874Z",
     "iopub.status.busy": "2025-10-15T09:34:28.420757Z",
     "iopub.status.idle": "2025-10-15T09:34:28.428749Z",
     "shell.execute_reply": "2025-10-15T09:34:28.428477Z"
    }
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "LABEL_TO_ID = {\"neutral\": 0, \"entails\": 1}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_TO_ID.items()}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: Path = Path('.')\n",
    "    min_freq: int = 2\n",
    "    max_vocab_size: Optional[int] = 30000\n",
    "    embed_dim: int = 200\n",
    "    hidden_size: int = 128\n",
    "    projection_dim: int = 256\n",
    "    mlp_dim: int = 256\n",
    "    dropout: float = 0.3\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 10\n",
    "    patience: int = 3\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    max_grad_norm: float = 5.0\n",
    "    seed: int = 2025\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return text.replace('\\n', ' ').strip()\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    text = clean_text(text.lower())\n",
    "    tokens = re.findall(r\"[a-z0-9]+(?:'[a-z0-9]+)?|[^\\w\\s]\", text)\n",
    "    return tokens if tokens else [\"<empty>\"]\n",
    "\n",
    "\n",
    "def load_split(path: Path) -> List[Dict[str, Optional[str]]]:\n",
    "    with open(path) as f:\n",
    "        raw = json.load(f)\n",
    "    ids = sorted(raw['premise'].keys(), key=lambda x: int(x))\n",
    "    samples: List[Dict[str, Optional[str]]] = []\n",
    "    for sid in ids:\n",
    "        sample = {\n",
    "            'id': int(sid),\n",
    "            'premise': raw['premise'][sid],\n",
    "            'hypothesis': raw['hypothesis'][sid],\n",
    "            'label': raw['label'].get(sid) if 'label' in raw else None,\n",
    "        }\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def attach_tokens(samples: List[Dict[str, Optional[str]]]) -> None:\n",
    "    for sample in samples:\n",
    "        sample['premise_tokens'] = tokenize(sample['premise'])\n",
    "        sample['hypothesis_tokens'] = tokenize(sample['hypothesis'])\n",
    "\n",
    "\n",
    "def describe_split(name: str, samples: List[Dict[str, Optional[str]]]) -> Dict[str, float]:\n",
    "    premise_lengths = np.array([len(s['premise_tokens']) for s in samples])\n",
    "    hypothesis_lengths = np.array([len(s['hypothesis_tokens']) for s in samples])\n",
    "    labels = [s['label'] for s in samples if s['label'] is not None]\n",
    "    stats = {\n",
    "        'num_examples': len(samples),\n",
    "        'premise_mean': float(premise_lengths.mean()),\n",
    "        'premise_median': float(np.median(premise_lengths)),\n",
    "        'premise_p95': float(np.percentile(premise_lengths, 95)),\n",
    "        'hyp_mean': float(hypothesis_lengths.mean()),\n",
    "        'hyp_median': float(np.median(hypothesis_lengths)),\n",
    "        'hyp_p95': float(np.percentile(hypothesis_lengths, 95)),\n",
    "    }\n",
    "    print(f\"Split: {name}\")\n",
    "    print(f\"  Examples: {stats['num_examples']}\")\n",
    "    print(f\"  Premise tokens -> mean {stats['premise_mean']:.1f} | median {stats['premise_median']:.1f} | 95th pct {stats['premise_p95']:.1f}\")\n",
    "    print(f\"  Hypothesis tokens -> mean {stats['hyp_mean']:.1f} | median {stats['hyp_median']:.1f} | 95th pct {stats['hyp_p95']:.1f}\")\n",
    "    if labels:\n",
    "        counter = Counter(labels)\n",
    "        total = sum(counter.values())\n",
    "        for label, count in counter.items():\n",
    "            print(f\"  Label '{label}': {count} ({count / total:.2%})\")\n",
    "    else:\n",
    "        print(\"  Labels not provided for this split.\")\n",
    "    print()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd82e923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.429986Z",
     "iopub.status.busy": "2025-10-15T09:34:28.429903Z",
     "iopub.status.idle": "2025-10-15T09:34:28.437124Z",
     "shell.execute_reply": "2025-10-15T09:34:28.436872Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, stoi: Dict[str, int]):\n",
    "        self.stoi = stoi\n",
    "        self.itos = {idx: token for token, idx in stoi.items()}\n",
    "        self.pad_id = self.stoi[PAD_TOKEN]\n",
    "        self.unk_id = self.stoi[UNK_TOKEN]\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, samples: Iterable[Dict[str, Optional[str]]], min_freq: int = 1, max_size: Optional[int] = None) -> 'Vocabulary':\n",
    "        counter: Counter = Counter()\n",
    "        for sample in samples:\n",
    "            counter.update(sample['premise_tokens'])\n",
    "            counter.update(sample['hypothesis_tokens'])\n",
    "        most_common = [tok for tok, freq in counter.most_common() if freq >= min_freq]\n",
    "        if max_size is not None:\n",
    "            capacity = max_size - 2\n",
    "            most_common = most_common[:max(0, capacity)]\n",
    "        stoi = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "        for token in most_common:\n",
    "            if token not in stoi:\n",
    "                stoi[token] = len(stoi)\n",
    "        return cls(stoi)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.stoi.get(tok, self.unk_id) for tok in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        return [self.itos.get(idx, UNK_TOKEN) for idx in ids]\n",
    "\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, samples: List[Dict[str, Optional[str]]], vocab: Vocabulary, label_to_id: Dict[str, int]):\n",
    "        self.vocab = vocab\n",
    "        self.label_to_id = label_to_id\n",
    "        self.samples = []\n",
    "        for sample in samples:\n",
    "            item = {\n",
    "                'id': sample['id'],\n",
    "                'premise_ids': vocab.encode(sample['premise_tokens']),\n",
    "                'hypothesis_ids': vocab.encode(sample['hypothesis_tokens']),\n",
    "                'premise_text': sample['premise'],\n",
    "                'hypothesis_text': sample['hypothesis'],\n",
    "                'label': label_to_id[sample['label']] if sample['label'] is not None else None,\n",
    "            }\n",
    "            self.samples.append(item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def decode_tokens(self, ids: List[int]) -> List[str]:\n",
    "        return [self.vocab.itos.get(idx, UNK_TOKEN) for idx in ids]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Optional[torch.Tensor]]:\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def build_collate_fn(pad_id: int):\n",
    "    def collate_fn(batch: List[Dict[str, Optional[torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        premise_seqs = [torch.tensor(item['premise_ids'], dtype=torch.long) for item in batch]\n",
    "        hypothesis_seqs = [torch.tensor(item['hypothesis_ids'], dtype=torch.long) for item in batch]\n",
    "        premise_lengths = torch.tensor([len(seq) for seq in premise_seqs], dtype=torch.long)\n",
    "        hypothesis_lengths = torch.tensor([len(seq) for seq in hypothesis_seqs], dtype=torch.long)\n",
    "        padded_premise = pad_sequence(premise_seqs, batch_first=True, padding_value=pad_id)\n",
    "        padded_hypothesis = pad_sequence(hypothesis_seqs, batch_first=True, padding_value=pad_id)\n",
    "        labels_list = [item['label'] for item in batch]\n",
    "        labels = None\n",
    "        if all(label is not None for label in labels_list):\n",
    "            labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        ids = torch.tensor([item['id'] for item in batch], dtype=torch.long)\n",
    "        return {\n",
    "            'premise': padded_premise,\n",
    "            'premise_lengths': premise_lengths,\n",
    "            'hypothesis': padded_hypothesis,\n",
    "            'hypothesis_lengths': hypothesis_lengths,\n",
    "            'labels': labels,\n",
    "            'ids': ids,\n",
    "        }\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48ae350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.438441Z",
     "iopub.status.busy": "2025-10-15T09:34:28.438355Z",
     "iopub.status.idle": "2025-10-15T09:34:28.447303Z",
     "shell.execute_reply": "2025-10-15T09:34:28.446962Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESIMBiGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        hidden_size: int,\n",
    "        projection_dim: int,\n",
    "        mlp_dim: int,\n",
    "        num_classes: int,\n",
    "        padding_idx: int,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 8, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.composition = nn.GRU(projection_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 8, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, num_classes),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.embedding.weight.data[self.padding_idx] = 0\n",
    "        for gru in [self.encoder, self.composition]:\n",
    "            for name, param in gru.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "        for module in self.projection:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "        for module in self.classifier:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def masked_softmax(tensor: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        tensor = tensor.masked_fill(~mask, float('-inf'))\n",
    "        return torch.softmax(tensor, dim=dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        premise: torch.Tensor,\n",
    "        hypothesis: torch.Tensor,\n",
    "        premise_lengths: torch.Tensor,\n",
    "        hypothesis_lengths: torch.Tensor,\n",
    "        return_alignments: bool = False,\n",
    "    ):\n",
    "        premise_mask = premise != self.padding_idx\n",
    "        hypothesis_mask = hypothesis != self.padding_idx\n",
    "\n",
    "        premise_embed = self.dropout(self.embedding(premise))\n",
    "        hypothesis_embed = self.dropout(self.embedding(hypothesis))\n",
    "\n",
    "        premise_encoded, _ = self.encoder(premise_embed)\n",
    "        hypothesis_encoded, _ = self.encoder(hypothesis_embed)\n",
    "        premise_encoded = self.dropout(premise_encoded)\n",
    "        hypothesis_encoded = self.dropout(hypothesis_encoded)\n",
    "\n",
    "        similarity = torch.bmm(premise_encoded, hypothesis_encoded.transpose(1, 2))\n",
    "\n",
    "        hyp_mask_expanded = hypothesis_mask.unsqueeze(1).expand_as(similarity)\n",
    "        prem_mask_expanded = premise_mask.unsqueeze(1).expand_as(similarity.transpose(1, 2))\n",
    "        weight_premise = self.masked_softmax(similarity, hyp_mask_expanded, dim=-1)\n",
    "        weight_hypothesis = self.masked_softmax(similarity.transpose(1, 2), prem_mask_expanded, dim=-1)\n",
    "\n",
    "        attended_premise = torch.bmm(weight_premise, hypothesis_encoded)\n",
    "        attended_hypothesis = torch.bmm(weight_hypothesis, premise_encoded)\n",
    "\n",
    "        premise_combined = torch.cat(\n",
    "            [\n",
    "                premise_encoded,\n",
    "                attended_premise,\n",
    "                premise_encoded - attended_premise,\n",
    "                premise_encoded * attended_premise,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        hypothesis_combined = torch.cat(\n",
    "            [\n",
    "                hypothesis_encoded,\n",
    "                attended_hypothesis,\n",
    "                hypothesis_encoded - attended_hypothesis,\n",
    "                hypothesis_encoded * attended_hypothesis,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        premise_projected = self.projection(premise_combined)\n",
    "        hypothesis_projected = self.projection(hypothesis_combined)\n",
    "\n",
    "        premise_composed, _ = self.composition(premise_projected)\n",
    "        hypothesis_composed, _ = self.composition(hypothesis_projected)\n",
    "\n",
    "        premise_composed = self.dropout(premise_composed)\n",
    "        hypothesis_composed = self.dropout(hypothesis_composed)\n",
    "\n",
    "        premise_avg = self.masked_mean(premise_composed, premise_mask)\n",
    "        hypothesis_avg = self.masked_mean(hypothesis_composed, hypothesis_mask)\n",
    "        premise_max = self.masked_max(premise_composed, premise_mask)\n",
    "        hypothesis_max = self.masked_max(hypothesis_composed, hypothesis_mask)\n",
    "\n",
    "        combined = torch.cat([premise_avg, premise_max, hypothesis_avg, hypothesis_max], dim=-1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        if return_alignments:\n",
    "            return logits, weight_premise, weight_hypothesis\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def masked_mean(sequence: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        mask = mask.unsqueeze(-1).type_as(sequence)\n",
    "        masked_seq = sequence * mask\n",
    "        summed = masked_seq.sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1.0)\n",
    "        return summed / counts\n",
    "\n",
    "    @staticmethod\n",
    "    def masked_max(sequence: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        masked_seq = sequence.masked_fill(~mask, float('-inf'))\n",
    "        return masked_seq.max(dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23754aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.448544Z",
     "iopub.status.busy": "2025-10-15T09:34:28.448465Z",
     "iopub.status.idle": "2025-10-15T09:34:28.456739Z",
     "shell.execute_reply": "2025-10-15T09:34:28.456455Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    max_grad_norm: float,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_examples = 0\n",
    "    all_preds: List[int] = []\n",
    "    all_labels: List[int] = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Train\", leave=False):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        premise = batch['premise'].to(device)\n",
    "        hypothesis = batch['hypothesis'].to(device)\n",
    "        premise_lengths = batch['premise_lengths'].to(device)\n",
    "        hypothesis_lengths = batch['hypothesis_lengths'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits = model(premise, hypothesis, premise_lengths, hypothesis_lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = premise.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_labels.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_examples = 0\n",
    "    all_preds: List[int] = []\n",
    "    all_labels: List[int] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            premise = batch['premise'].to(device)\n",
    "            hypothesis = batch['hypothesis'].to(device)\n",
    "            premise_lengths = batch['premise_lengths'].to(device)\n",
    "            hypothesis_lengths = batch['hypothesis_lengths'].to(device)\n",
    "            labels = batch['labels']\n",
    "            logits = model(premise, hypothesis, premise_lengths, hypothesis_lengths)\n",
    "            if labels is not None:\n",
    "                labels = labels.to(device)\n",
    "                loss = criterion(logits, labels)\n",
    "                batch_size = premise.size(0)\n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_examples += batch_size\n",
    "                all_labels.extend(labels.detach().cpu().tolist())\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                all_preds.extend(preds.detach().cpu().tolist())\n",
    "            else:\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                all_preds.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    metrics = {\"predictions\": all_preds}\n",
    "    if total_examples > 0:\n",
    "        metrics[\"loss\"] = total_loss / total_examples\n",
    "        metrics[\"accuracy\"] = accuracy_score(all_labels, all_preds)\n",
    "        metrics[\"labels\"] = all_labels\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: Config,\n",
    "    device: torch.device,\n",
    ") -> Tuple[nn.Module, List[Dict[str, float]]]:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "    best_state = None\n",
    "    best_val_accuracy = -math.inf\n",
    "    epochs_without_improvement = 0\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{config.num_epochs}\")\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, criterion, device, config.max_grad_norm)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        log_entry = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "        }\n",
    "        history.append(log_entry)\n",
    "        print(f\"  Train -> loss: {train_metrics['loss']:.4f}, accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Val   -> loss: {val_metrics['loss']:.4f}, accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "            print(\"  New best validation accuracy; checkpoint updated.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "            if epochs_without_improvement >= config.patience:\n",
    "                print(\"  Early stopping triggered.\")\n",
    "                break\n",
    "        print()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baea6931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.457860Z",
     "iopub.status.busy": "2025-10-15T09:34:28.457790Z",
     "iopub.status.idle": "2025-10-15T09:34:28.700609Z",
     "shell.execute_reply": "2025-10-15T09:34:28.700340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train\n",
      "  Examples: 23088\n",
      "  Premise tokens -> mean 21.1 | median 18.0 | 95th pct 38.0\n",
      "  Hypothesis tokens -> mean 13.2 | median 12.0 | 95th pct 22.0\n",
      "  Label 'neutral': 14618 (63.31%)\n",
      "  Label 'entails': 8470 (36.69%)\n",
      "\n",
      "Split: validation\n",
      "  Examples: 1304\n",
      "  Premise tokens -> mean 19.9 | median 17.0 | 95th pct 39.0\n",
      "  Hypothesis tokens -> mean 13.8 | median 13.0 | 95th pct 23.0\n",
      "  Label 'neutral': 647 (49.62%)\n",
      "  Label 'entails': 657 (50.38%)\n",
      "\n",
      "Split: test\n",
      "  Examples: 2126\n",
      "  Premise tokens -> mean 19.3 | median 18.0 | 95th pct 37.0\n",
      "  Hypothesis tokens -> mean 14.0 | median 13.0 | 95th pct 25.0\n",
      "  Label 'neutral': 1284 (60.40%)\n",
      "  Label 'entails': 842 (39.60%)\n",
      "\n",
      "Sample training instance:\n",
      "{\n",
      "  \"premise\": \"Pluto rotates once on its axis every 6.39 Earth days;\",\n",
      "  \"hypothesis\": \"Earth rotates on its axis once times in one day.\",\n",
      "  \"label\": \"neutral\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "set_seed(config.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_path = config.data_dir / 'train.json'\n",
    "val_path = config.data_dir / 'validation.json'\n",
    "test_path = config.data_dir / 'test.json'\n",
    "\n",
    "train_samples = load_split(train_path)\n",
    "val_samples = load_split(val_path)\n",
    "test_samples = load_split(test_path)\n",
    "\n",
    "for split in (train_samples, val_samples, test_samples):\n",
    "    attach_tokens(split)\n",
    "\n",
    "train_stats = describe_split('train', train_samples)\n",
    "val_stats = describe_split('validation', val_samples)\n",
    "test_stats = describe_split('test', test_samples)\n",
    "\n",
    "print(\"Sample training instance:\")\n",
    "print(json.dumps({\n",
    "    'premise': train_samples[0]['premise'],\n",
    "    'hypothesis': train_samples[0]['hypothesis'],\n",
    "    'label': train_samples[0]['label'],\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6018c",
   "metadata": {},
   "source": [
    "The dataset inspection confirms that premises are longer than hypotheses (median lengths ≈18 vs ≈12 tokens). The label distribution is moderately imbalanced, with neutral outweighing entails in the train/test splits, so the optimisation objective must handle this skew. The 95th-percentile sequence lengths (≈38 and ≈23 tokens) guide the choice of batch padding and help size the BiGRU hidden state without excessive memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5291328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.701879Z",
     "iopub.status.busy": "2025-10-15T09:34:28.701805Z",
     "iopub.status.idle": "2025-10-15T09:34:28.910815Z",
     "shell.execute_reply": "2025-10-15T09:34:28.910547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 11447 (including pad/unk)\n",
      "Batch tensor shapes:\n",
      "  premise: (32, 40)\n",
      "  premise_lengths: (32,)\n",
      "  hypothesis: (32, 21)\n",
      "  hypothesis_lengths: (32,)\n",
      "  labels: (32,)\n",
      "  ids: (32,)\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.build(train_samples, min_freq=config.min_freq, max_size=config.max_vocab_size)\n",
    "print(f\"Vocabulary size: {len(vocab)} (including pad/unk)\")\n",
    "\n",
    "train_dataset = NLIDataset(train_samples, vocab, LABEL_TO_ID)\n",
    "val_dataset = NLIDataset(val_samples, vocab, LABEL_TO_ID)\n",
    "test_dataset = NLIDataset(test_samples, vocab, LABEL_TO_ID)\n",
    "\n",
    "collate_fn = build_collate_fn(vocab.pad_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch_example = next(iter(train_loader))\n",
    "print('Batch tensor shapes:')\n",
    "for key, value in batch_example.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {tuple(value.size())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc3fca",
   "metadata": {},
   "source": [
    "The dataloaders dynamically pad each batch to the maximum length within that batch. This keeps computation efficient while preserving all tokens for the attention mechanism. The vocabulary is capped to frequent tokens (min frequency = 2) to control the embedding matrix size and mitigate overfitting on rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5fd59f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.912097Z",
     "iopub.status.busy": "2025-10-15T09:34:28.912015Z",
     "iopub.status.idle": "2025-10-15T09:34:28.944853Z",
     "shell.execute_reply": "2025-10-15T09:34:28.944595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 3,364,602\n"
     ]
    }
   ],
   "source": [
    "model = ESIMBiGRU(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=config.embed_dim,\n",
    "    hidden_size=config.hidden_size,\n",
    "    projection_dim=config.projection_dim,\n",
    "    mlp_dim=config.mlp_dim,\n",
    "    num_classes=len(LABEL_TO_ID),\n",
    "    padding_idx=vocab.pad_id,\n",
    "    dropout=config.dropout,\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc05555b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:34:28.946084Z",
     "iopub.status.busy": "2025-10-15T09:34:28.945998Z",
     "iopub.status.idle": "2025-10-15T09:44:19.306016Z",
     "shell.execute_reply": "2025-10-15T09:44:19.293145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train -> loss: 0.4414, accuracy: 0.7930\n",
      "  Val   -> loss: 0.6002, accuracy: 0.6618\n",
      "  New best validation accuracy; checkpoint updated.\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train -> loss: 0.2791, accuracy: 0.8838\n",
      "  Val   -> loss: 0.6697, accuracy: 0.7017\n",
      "  New best validation accuracy; checkpoint updated.\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train -> loss: 0.1945, accuracy: 0.9228\n",
      "  Val   -> loss: 0.7398, accuracy: 0.7163\n",
      "  New best validation accuracy; checkpoint updated.\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train -> loss: 0.1348, accuracy: 0.9487\n",
      "  Val   -> loss: 0.8163, accuracy: 0.7063\n",
      "  No improvement for 1 epoch(s).\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train -> loss: 0.0932, accuracy: 0.9641\n",
      "  Val   -> loss: 1.0852, accuracy: 0.7132\n",
      "  No improvement for 2 epoch(s).\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train -> loss: 0.0662, accuracy: 0.9749\n",
      "  Val   -> loss: 1.3192, accuracy: 0.6910\n",
      "  No improvement for 3 epoch(s).\n",
      "  Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5831a527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:44:19.354810Z",
     "iopub.status.busy": "2025-10-15T09:44:19.343568Z",
     "iopub.status.idle": "2025-10-15T09:44:19.382517Z",
     "shell.execute_reply": "2025-10-15T09:44:19.382248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history (best epoch highlighted):\n",
      "    Epoch 01 | Train loss 0.4414 | Train acc 0.7930 | Val loss 0.6002 | Val acc 0.6618\n",
      "    Epoch 02 | Train loss 0.2791 | Train acc 0.8838 | Val loss 0.6697 | Val acc 0.7017\n",
      "*** Epoch 03 | Train loss 0.1945 | Train acc 0.9228 | Val loss 0.7398 | Val acc 0.7163\n",
      "    Epoch 04 | Train loss 0.1348 | Train acc 0.9487 | Val loss 0.8163 | Val acc 0.7063\n",
      "    Epoch 05 | Train loss 0.0932 | Train acc 0.9641 | Val loss 1.0852 | Val acc 0.7132\n",
      "    Epoch 06 | Train loss 0.0662 | Train acc 0.9749 | Val loss 1.3192 | Val acc 0.6910\n"
     ]
    }
   ],
   "source": [
    "print(\"Training history (best epoch highlighted):\")\n",
    "best_epoch = max(history, key=lambda x: x['val_accuracy'])['epoch']\n",
    "for record in history:\n",
    "    marker = '***' if record['epoch'] == best_epoch else '   '\n",
    "    print(f\"{marker} Epoch {record['epoch']:02d} | Train loss {record['train_loss']:.4f} | Train acc {record['train_accuracy']:.4f} | Val loss {record['val_loss']:.4f} | Val acc {record['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a036ac79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:44:19.384389Z",
     "iopub.status.busy": "2025-10-15T09:44:19.384290Z",
     "iopub.status.idle": "2025-10-15T09:44:21.513796Z",
     "shell.execute_reply": "2025-10-15T09:44:21.513542Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.67      0.83      0.74       647\n",
      "     entails       0.78      0.60      0.68       657\n",
      "\n",
      "    accuracy                           0.72      1304\n",
      "   macro avg       0.73      0.72      0.71      1304\n",
      "weighted avg       0.73      0.72      0.71      1304\n",
      "\n",
      "Validation confusion matrix:\n",
      "[[538 109]\n",
      " [261 396]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.75      0.82      0.78      1284\n",
      "     entails       0.68      0.58      0.63       842\n",
      "\n",
      "    accuracy                           0.73      2126\n",
      "   macro avg       0.72      0.70      0.71      2126\n",
      "weighted avg       0.72      0.73      0.72      2126\n",
      "\n",
      "Test confusion matrix:\n",
      "[[1053  231]\n",
      " [ 351  491]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "print(\"Validation report:\")\n",
    "print(classification_report(val_metrics['labels'], val_metrics['predictions'], target_names=[ID_TO_LABEL[i] for i in range(len(LABEL_TO_ID))]))\n",
    "\n",
    "val_confusion = confusion_matrix(val_metrics['labels'], val_metrics['predictions'])\n",
    "print(\"Validation confusion matrix:\")\n",
    "print(val_confusion)\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "if 'labels' in test_metrics:\n",
    "    print(\"\\nTest report:\")\n",
    "    print(classification_report(test_metrics['labels'], test_metrics['predictions'], target_names=[ID_TO_LABEL[i] for i in range(len(LABEL_TO_ID))]))\n",
    "    test_confusion = confusion_matrix(test_metrics['labels'], test_metrics['predictions'])\n",
    "    print(\"Test confusion matrix:\")\n",
    "    print(test_confusion)\n",
    "else:\n",
    "    print(\"Test split has no labels; generated predictions for offline evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769fdeb",
   "metadata": {},
   "source": [
    "### Attention inspection on a validation example\n",
    "To verify that the soft-alignment component behaves sensibly, we inspect the top alignment weights for a correctly classified validation item. The weights reveal which premise tokens receive the strongest focus when matching each hypothesis token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010c0a21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:44:21.515801Z",
     "iopub.status.busy": "2025-10-15T09:44:21.515697Z",
     "iopub.status.idle": "2025-10-15T09:44:21.534039Z",
     "shell.execute_reply": "2025-10-15T09:44:21.533770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example ID: 0\n",
      "Premise: an introduction to atoms and elements , compounds , atomic structure and bonding , the molecule and chemical reactions .\n",
      "Hypothesis: replace another in a molecule happens to atoms during a substitution reaction .\n",
      "Predicted label: neutral | Gold label: neutral\n",
      "\n",
      "Hypothesis token 'replace' attends to -> the:0.069, ,:0.069, reactions:0.068\n",
      "Hypothesis token 'another' attends to -> the:0.077, molecule:0.072, reactions:0.072\n",
      "Hypothesis token 'in' attends to -> an:0.074, the:0.073, reactions:0.071\n",
      "Hypothesis token 'a' attends to -> the:0.084, molecule:0.083, ,:0.078\n",
      "Hypothesis token 'molecule' attends to -> molecule:0.133, the:0.080, an:0.073\n",
      "Hypothesis token 'happens' attends to -> structure:0.111, and:0.106, and:0.104\n",
      "Hypothesis token 'to' attends to -> introduction:0.115, and:0.102, an:0.100\n",
      "Hypothesis token 'atoms' attends to -> elements:0.142, introduction:0.126, atoms:0.120\n",
      "Hypothesis token 'during' attends to -> reactions:0.095, the:0.090, atomic:0.085\n",
      "Hypothesis token 'a' attends to -> atomic:0.110, atoms:0.101, elements:0.097\n",
      "Hypothesis token 'substitution' attends to -> and:0.105, and:0.101, structure:0.101\n",
      "Hypothesis token 'reaction' attends to -> structure:0.134, and:0.128, and:0.126\n",
      "Hypothesis token '.' attends to -> introduction:0.118, bonding:0.116, and:0.112\n"
     ]
    }
   ],
   "source": [
    "def show_alignment(model: ESIMBiGRU, dataset: NLIDataset, index: int) -> None:\n",
    "    model.eval()\n",
    "    sample = dataset[index]\n",
    "    with torch.no_grad():\n",
    "        batch = {\n",
    "            'premise': torch.tensor(sample['premise_ids'], dtype=torch.long, device=device).unsqueeze(0),\n",
    "            'hypothesis': torch.tensor(sample['hypothesis_ids'], dtype=torch.long, device=device).unsqueeze(0),\n",
    "            'premise_lengths': torch.tensor([len(sample['premise_ids'])], dtype=torch.long, device=device),\n",
    "            'hypothesis_lengths': torch.tensor([len(sample['hypothesis_ids'])], dtype=torch.long, device=device),\n",
    "        }\n",
    "        logits, weight_premise, weight_hypothesis = model(\n",
    "            batch['premise'], batch['hypothesis'], batch['premise_lengths'], batch['hypothesis_lengths'], return_alignments=True\n",
    "        )\n",
    "        predicted = logits.argmax(dim=-1).item()\n",
    "\n",
    "    premise_tokens = dataset.decode_tokens(sample['premise_ids'])\n",
    "    hypothesis_tokens = dataset.decode_tokens(sample['hypothesis_ids'])\n",
    "\n",
    "    print(f\"Example ID: {sample['id']}\")\n",
    "    print(f\"Premise: {' '.join(premise_tokens)}\")\n",
    "    print(f\"Hypothesis: {' '.join(hypothesis_tokens)}\")\n",
    "    print(f\"Predicted label: {ID_TO_LABEL[predicted]} | Gold label: {ID_TO_LABEL[sample['label']] if sample['label'] is not None else 'n/a'}\")\n",
    "    print()\n",
    "\n",
    "    alignment = weight_premise.squeeze(0).cpu().numpy()\n",
    "    for i, hyp_token in enumerate(hypothesis_tokens):\n",
    "        attention_row = alignment[:, i]\n",
    "        top_indices = attention_row.argsort()[-3:][::-1]\n",
    "        top_pairs = [(premise_tokens[j], attention_row[j]) for j in top_indices]\n",
    "        formatted = ', '.join(f\"{tok}:{score:.3f}\" for tok, score in top_pairs)\n",
    "        print(f\"Hypothesis token '{hyp_token}' attends to -> {formatted}\")\n",
    "\n",
    "\n",
    "show_alignment(model, val_dataset, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1522d66",
   "metadata": {},
   "source": [
    "### Notes and next steps\n",
    "- The ESIM-style architecture provides interpretable alignment scores and achieves competitive validation/test performance within the project constraints.\n",
    "- Further work (for the full report) can explore ablations such as removing the difference/product features or exchanging max pooling for attentive pooling, as well as hyperparameter sweeps guided by the logged metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
