{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70d373b",
   "metadata": {},
   "source": [
    "# Model C – Lightweight Transformer Cross-Encoder\n",
    "\n",
    "This notebook implements the Model C architecture described in the project brief: a lightweight transformer trained from scratch to act as a cross-encoder for the science NLI task. The solution adheres to the specification constraints (no pretrained checkpoints, transformer depth within compute limits) and is designed for transparency and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d1063d",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Run the cell below once in a fresh runtime to install the Python packages required by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet numpy torch --index-url https://download.pytorch.org/whl/cpu\n",
    "# tqdm installs automatically with torch on most setups, but include explicitly for completeness.\n",
    "%pip install --quiet tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157cbf82",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "- Load the provided NLI JSON splits and normalise them into record-based structures.\n",
    "- Build a vocabulary from the training split only and construct learned token, segment, and position embeddings with explicit / markers.\n",
    "- Instantiate a shallow multi-head self-attention encoder that processes the concatenated premise–hypothesis sequence.\n",
    "- Optimise with cross-entropy loss on entailment vs neutral, monitor validation metrics, and log per-class accuracies.\n",
    "- Provide utilities for test-set inference and exporting the training history for inclusion in the project submission log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7f79b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\UWA Master Of Information Technology\\Semester 3 2025\\CITS4012 Natural Language Processing\\Project\\CITS4012-Group-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:  # pragma: no cover\n",
    "    tqdm = lambda x, **kwargs: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e673387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Configuration: {'data_dir': WindowsPath('..'), 'train_file': 'train.json', 'val_file': 'validation.json', 'test_file': 'test.json', 'max_length': 256, 'min_freq': 2, 'embedding_dim': 256, 'num_heads': 4, 'num_layers': 3, 'feedforward_dim': 512, 'dropout': 0.1, 'batch_size': 32, 'num_epochs': 8, 'lr': 0.0002, 'weight_decay': 0.0001, 'max_grad_norm': 1.0, 'warmup_ratio': 0.1, 'seed': 2025}\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelCConfig:\n",
    "    data_dir: Path = Path(\"..\")\n",
    "    train_file: str = \"train.json\"\n",
    "    val_file: str = \"validation.json\"\n",
    "    test_file: str = \"test.json\"\n",
    "    max_length: int = 256\n",
    "    min_freq: int = 2\n",
    "    embedding_dim: int = 256\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 3\n",
    "    feedforward_dim: int = 512\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 8\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_ratio: float = 0.1\n",
    "    seed: int = 2025\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "config = ModelCConfig()\n",
    "set_seed(config.seed)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Configuration: {asdict(config)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92781a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_PATTERN = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[^\\w\\s]\")\n",
    "\n",
    "\n",
    "def basic_tokenize(text: str) -> List[str]:\n",
    "    text = text.strip().lower()\n",
    "    return TOKEN_PATTERN.findall(text)\n",
    "\n",
    "\n",
    "def load_split(file_path: Path) -> List[Dict[str, Optional[str]]]:\n",
    "    with file_path.open() as stream:\n",
    "        payload = json.load(stream)\n",
    "    keys = sorted(payload[\"premise\"].keys(), key=int)\n",
    "    label_data = payload.get(\"label\")\n",
    "    records: List[Dict[str, Optional[str]]] = []\n",
    "    for key in keys:\n",
    "        label: Optional[str] = None\n",
    "        if label_data is not None:\n",
    "            if isinstance(label_data, dict):\n",
    "                label = label_data.get(key)\n",
    "            else:\n",
    "                label = label_data[int(key)]\n",
    "        records.append(\n",
    "            {\n",
    "                \"id\": key,\n",
    "                \"premise\": payload[\"premise\"][key],\n",
    "                \"hypothesis\": payload[\"hypothesis\"][key],\n",
    "                \"label\": label,\n",
    "            }\n",
    "        )\n",
    "    return records\n",
    "\n",
    "\n",
    "def preprocess_records(records: List[Dict[str, Optional[str]]]) -> None:\n",
    "    for item in records:\n",
    "        item[\"premise_tokens\"] = basic_tokenize(item[\"premise\"])\n",
    "        item[\"hypothesis_tokens\"] = basic_tokenize(item[\"hypothesis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98790526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 23088 samples\n",
      "Validation split: 1304 samples\n",
      "Test split: 2126 samples\n",
      "Label distribution (train): Counter({'neutral': 14618, 'entails': 8470})\n",
      "Train token count (premise+hypothesis) – avg: 34.3, max: 14657, 95th percentile: 54\n",
      "Validation token count (premise+hypothesis) – avg: 33.7, max: 81, 95th percentile: 56\n",
      "Test token count (premise+hypothesis) – avg: 33.4, max: 74, 95th percentile: 54\n"
     ]
    }
   ],
   "source": [
    "data_root = config.data_dir\n",
    "train_records = load_split(data_root / config.train_file)\n",
    "val_records = load_split(data_root / config.val_file)\n",
    "test_records = load_split(data_root / config.test_file)\n",
    "\n",
    "for split_name, split_records in [(\"train\", train_records), (\"validation\", val_records), (\"test\", test_records)]:\n",
    "    preprocess_records(split_records)\n",
    "    print(f\"{split_name.title()} split: {len(split_records)} samples\")\n",
    "\n",
    "label_counts = Counter(rec[\"label\"] for rec in train_records if rec[\"label\"] is not None)\n",
    "print(\"Label distribution (train):\", label_counts)\n",
    "\n",
    "\n",
    "def describe_lengths(records: List[Dict[str, Optional[str]]], split_name: str) -> None:\n",
    "    lengths = [len(rec[\"premise_tokens\"]) + len(rec[\"hypothesis_tokens\"]) for rec in records]\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    print(\n",
    "        f\"{split_name.title()} token count (premise+hypothesis) – avg: {avg_len:.1f}, \"\n",
    "        f\"max: {max(lengths)}, 95th percentile: {np.percentile(lengths, 95):.0f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "describe_lengths(train_records, \"train\")\n",
    "describe_lengths(val_records, \"validation\")\n",
    "describe_lengths(test_records, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e741ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 11449\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_freq: int = 1):\n",
    "        self.min_freq = min_freq\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.sep_token = \"<sep>\"\n",
    "        self.special_tokens = [self.pad_token, self.unk_token, self.cls_token, self.sep_token]\n",
    "        self.token_to_idx: Dict[str, int] = {}\n",
    "        self.idx_to_token: List[str] = []\n",
    "\n",
    "    def build(self, records: Iterable[Dict[str, Optional[str]]]) -> None:\n",
    "        counter = Counter()\n",
    "        for record in records:\n",
    "            counter.update(record[\"premise_tokens\"])\n",
    "            counter.update(record[\"hypothesis_tokens\"])\n",
    "        tokens = [token for token, freq in counter.items() if freq >= self.min_freq]\n",
    "        ordered_tokens = sorted(tokens)\n",
    "        self.idx_to_token = list(self.special_tokens) + ordered_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self.token_to_idx[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def cls_id(self) -> int:\n",
    "        return self.token_to_idx[self.cls_token]\n",
    "\n",
    "    @property\n",
    "    def sep_id(self) -> int:\n",
    "        return self.token_to_idx[self.sep_token]\n",
    "\n",
    "    @property\n",
    "    def unk_id(self) -> int:\n",
    "        return self.token_to_idx[self.unk_token]\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.token_to_idx.get(token, self.unk_id) for token in tokens]\n",
    "\n",
    "\n",
    "vocab = Vocabulary(min_freq=config.min_freq)\n",
    "vocab.build(train_records)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fed210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 722 | Validation batches: 41 | Test batches: 67\n"
     ]
    }
   ],
   "source": [
    "LABEL_TO_ID = {\"entails\": 0, \"neutral\": 1}\n",
    "ID_TO_LABEL = {idx: label for label, idx in LABEL_TO_ID.items()}\n",
    "\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, records: List[Dict[str, Optional[str]]], vocab: Vocabulary, config: ModelCConfig):\n",
    "        self.records = records\n",
    "        self.vocab = vocab\n",
    "        self.config = config\n",
    "        self.has_labels = all(record[\"label\"] is not None for record in records)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def _build_input(self, record: Dict[str, Optional[str]]) -> Tuple[List[int], List[int], List[bool]]:\n",
    "        tokens = [self.vocab.cls_token]\n",
    "        segments = [0]\n",
    "        tokens.extend(record[\"premise_tokens\"])\n",
    "        segments.extend([1] * len(record[\"premise_tokens\"]))\n",
    "        tokens.append(self.vocab.sep_token)\n",
    "        segments.append(0)\n",
    "        tokens.extend(record[\"hypothesis_tokens\"])\n",
    "        segments.extend([2] * len(record[\"hypothesis_tokens\"]))\n",
    "\n",
    "        input_ids = self.vocab.encode(tokens)\n",
    "        attention_mask = [True] * len(input_ids)\n",
    "        max_len = self.config.max_length\n",
    "        if len(input_ids) > max_len:\n",
    "            input_ids = input_ids[:max_len]\n",
    "            segments = segments[:max_len]\n",
    "            attention_mask = attention_mask[:max_len]\n",
    "        else:\n",
    "            pad_needed = max_len - len(input_ids)\n",
    "            if pad_needed > 0:\n",
    "                input_ids += [self.vocab.pad_id] * pad_needed\n",
    "                segments += [0] * pad_needed\n",
    "                attention_mask += [False] * pad_needed\n",
    "        return input_ids, segments, attention_mask\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        record = self.records[idx]\n",
    "        input_ids, segment_ids, attention_mask = self._build_input(record)\n",
    "        label_idx = LABEL_TO_ID[record[\"label\"]] if self.has_labels else -1\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"segment_ids\": torch.tensor(segment_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.bool),\n",
    "            \"labels\": torch.tensor(label_idx, dtype=torch.long),\n",
    "            \"sample_id\": record[\"id\"],\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = NLIDataset(train_records, vocab, config)\n",
    "val_dataset = NLIDataset(val_records, vocab, config)\n",
    "test_dataset = NLIDataset(test_records, vocab, config)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"Train batches: {len(train_loader)} | Validation batches: {len(val_loader)} | Test batches: {len(test_loader)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eb028",
   "metadata": {},
   "source": [
    "### Transformer Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ebf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCrossEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, config: ModelCConfig, pad_id: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, config.embedding_dim, padding_idx=pad_id)\n",
    "        self.segment_embeddings = nn.Embedding(3, config.embedding_dim)\n",
    "        self.position_embeddings = nn.Embedding(config.max_length, config.embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.embedding_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, config.embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.embedding_dim, len(LABEL_TO_ID)),\n",
    "        )\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self) -> None:\n",
    "        nn.init.normal_(self.token_embeddings.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.segment_embeddings.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.position_embeddings.weight, mean=0.0, std=0.02)\n",
    "        for module in self.classifier:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, segment_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        embeddings = (\n",
    "            self.token_embeddings(input_ids)\n",
    "            + self.segment_embeddings(segment_ids)\n",
    "            + self.position_embeddings(position_ids)\n",
    "        )\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        key_padding_mask = ~attention_mask.to(dtype=torch.bool)\n",
    "        encoded = self.encoder(embeddings, src_key_padding_mask=key_padding_mask)\n",
    "        cls_representation = encoded[:, 0, :]\n",
    "        logits = self.classifier(self.dropout(cls_representation))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0414715",
   "metadata": {},
   "source": [
    "### Optimisation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00390679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLinearScheduler:\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, warmup_steps: int, total_steps: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = max(1, warmup_steps)\n",
    "        self.total_steps = max(1, total_steps)\n",
    "        self.num_steps = 0\n",
    "        self.base_lrs = [group[\"lr\"] for group in optimizer.param_groups]\n",
    "\n",
    "    def step(self) -> None:\n",
    "        self.num_steps += 1\n",
    "        for param_group, base_lr in zip(self.optimizer.param_groups, self.base_lrs):\n",
    "            if self.num_steps <= self.warmup_steps:\n",
    "                lr = base_lr * self.num_steps / self.warmup_steps\n",
    "            else:\n",
    "                decay_steps = self.total_steps - self.warmup_steps\n",
    "                lr = base_lr * max(0.0, (self.total_steps - self.num_steps) / max(1, decay_steps))\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def forward_batch(model: nn.Module, batch: Dict[str, torch.Tensor], device: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    segment_ids = batch[\"segment_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    logits = model(input_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "    return logits, labels\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[WarmupLinearScheduler],\n",
    "    criterion: nn.Module,\n",
    "    device: str,\n",
    "    max_grad_norm: float,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    progress = tqdm(data_loader, desc=\"Train\", leave=False)\n",
    "    for batch in progress:\n",
    "        optimizer.zero_grad()\n",
    "        logits, labels = forward_batch(model, batch, device)\n",
    "        if (labels < 0).all():\n",
    "            continue\n",
    "        valid_mask = labels >= 0\n",
    "        logits = logits[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        batch_size = labels.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += batch_size\n",
    "        if hasattr(progress, \"set_postfix\"):\n",
    "            progress.set_postfix({\"loss\": total_loss / total_examples, \"acc\": total_correct / total_examples})\n",
    "    return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, data_loader: DataLoader, criterion: nn.Module, device: str\n",
    ") -> Dict[str, object]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    label_totals: Counter = Counter()\n",
    "    label_correct: Counter = Counter()\n",
    "    all_preds: List[int] = []\n",
    "    all_labels: List[int] = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            logits, labels = forward_batch(model, batch, device)\n",
    "            valid_mask = labels >= 0\n",
    "            logits = logits[valid_mask]\n",
    "            labels = labels[valid_mask]\n",
    "            if labels.numel() == 0:\n",
    "                continue\n",
    "            batch_size = labels.size(0)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_examples += batch_size\n",
    "            label_list = labels.cpu().tolist()\n",
    "            pred_list = preds.cpu().tolist()\n",
    "            label_totals.update(label_list)\n",
    "            for label, pred in zip(label_list, pred_list):\n",
    "                if label == pred:\n",
    "                    label_correct.update([label])\n",
    "            all_labels.extend(label_list)\n",
    "            all_preds.extend(pred_list)\n",
    "    avg_loss = total_loss / total_examples\n",
    "    accuracy = total_correct / total_examples\n",
    "    per_label_accuracy = {\n",
    "        ID_TO_LABEL[label_id]: label_correct[label_id] / label_totals[label_id]\n",
    "        for label_id in label_totals\n",
    "    }\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"per_label_accuracy\": per_label_accuracy,\n",
    "        \"predictions\": all_preds,\n",
    "        \"targets\": all_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, config: ModelCConfig\n",
    ") -> Tuple[List[Dict[str, object]], float]:\n",
    "    device = config.device\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "    warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "    scheduler = WarmupLinearScheduler(optimizer, warmup_steps, total_steps) if total_steps > 0 else None\n",
    "\n",
    "    history: List[Dict[str, object]] = []\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, device, config.max_grad_norm\n",
    "        )\n",
    "        val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_loss\": val_metrics[\"loss\"],\n",
    "                \"val_acc\": val_metrics[\"accuracy\"],\n",
    "                \"val_per_label_acc\": val_metrics[\"per_label_accuracy\"],\n",
    "            }\n",
    "        )\n",
    "        if val_metrics[\"accuracy\"] > best_val_acc:\n",
    "            best_val_acc = val_metrics[\"accuracy\"]\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} acc={train_acc:.4f} \"\n",
    "            f\"| val_loss={val_metrics['loss']:.4f} acc={val_metrics['accuracy']:.4f}\"\n",
    "        )\n",
    "        print(f\"  Validation per-label accuracy: {val_metrics['per_label_accuracy']}\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return history, best_val_acc\n",
    "\n",
    "\n",
    "def predict_labels(model: nn.Module, data_loader: DataLoader, device: str) -> Tuple[List[str], List[int]]:\n",
    "    model.eval()\n",
    "    sample_ids: List[str] = []\n",
    "    predictions: List[int] = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            segment_ids = batch[\"segment_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            logits = model(input_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "            preds = logits.argmax(dim=-1).cpu().tolist()\n",
    "            predictions.extend(preds)\n",
    "            sample_ids.extend(batch[\"sample_id\"])\n",
    "    return sample_ids, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc751f",
   "metadata": {},
   "source": [
    "### Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2644ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\UWA Master Of Information Technology\\Semester 3 2025\\CITS4012 Natural Language Processing\\Project\\CITS4012-Group-Project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.5601 acc=0.7096 | val_loss=0.5715 acc=0.6948\n",
      "  Validation per-label accuracy: {'neutral': 0.7527047913446677, 'entails': 0.6377473363774734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=0.3593 acc=0.8488 | val_loss=0.7814 acc=0.6748\n",
      "  Validation per-label accuracy: {'neutral': 0.8918083462132921, 'entails': 0.4611872146118721}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_loss=0.2238 acc=0.9144 | val_loss=0.8391 acc=0.6917\n",
      "  Validation per-label accuracy: {'neutral': 0.8562596599690881, 'entails': 0.5296803652968036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train_loss=0.1265 acc=0.9560 | val_loss=1.3095 acc=0.6802\n",
      "  Validation per-label accuracy: {'neutral': 0.8361669242658424, 'entails': 0.5266362252663622}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train_loss=0.0649 acc=0.9808 | val_loss=1.7408 acc=0.6687\n",
      "  Validation per-label accuracy: {'neutral': 0.8670788253477589, 'entails': 0.4733637747336377}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train_loss=0.0309 acc=0.9913 | val_loss=1.8952 acc=0.7025\n",
      "  Validation per-label accuracy: {'neutral': 0.7619783616692427, 'entails': 0.6438356164383562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train_loss=0.0119 acc=0.9971 | val_loss=2.5904 acc=0.6910\n",
      "  Validation per-label accuracy: {'neutral': 0.7758887171561051, 'entails': 0.6073059360730594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train_loss=0.0051 acc=0.9987 | val_loss=2.9635 acc=0.6887\n",
      "  Validation per-label accuracy: {'neutral': 0.7959814528593508, 'entails': 0.5829528158295282}\n",
      "Best validation accuracy: 0.7025\n"
     ]
    }
   ],
   "source": [
    "model = TransformerCrossEncoder(vocab_size=len(vocab), config=config, pad_id=vocab.pad_id)\n",
    "training_history, best_val_acc = train_model(model, train_loader, val_loader, config)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee7fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "{'loss': 1.8951918625392796, 'accuracy': 0.7024539877300614, 'per_label_accuracy': {'neutral': 0.7619783616692427, 'entails': 0.6438356164383562}, 'predictions': [1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0], 'targets': [1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0]}\n",
      "Test metrics:\n",
      "{'loss': 1.8399478511101552, 'accuracy': 0.7083725305738476, 'per_label_accuracy': {'neutral': 0.7904984423676013, 'entails': 0.5831353919239906}, 'predictions': [1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1], 'targets': [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "val_metrics = evaluate_model(model, val_loader, criterion, config.device)\n",
    "print(\"Validation metrics:\")\n",
    "print(val_metrics)\n",
    "\n",
    "if test_dataset.has_labels:\n",
    "    test_metrics = evaluate_model(model, test_loader, criterion, config.device)\n",
    "    print(\"Test metrics:\")\n",
    "    print(test_metrics)\n",
    "else:\n",
    "    print(\"Test split has no ground-truth labels; generating predictions only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36006c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history saved to G:\\UWA Master Of Information Technology\\Semester 3 2025\\CITS4012 Natural Language Processing\\Project\\CITS4012-Group-Project\\modelC_history.json\n"
     ]
    }
   ],
   "source": [
    "history_path = Path(\"modelC_history.json\")\n",
    "history_path.write_text(json.dumps(training_history, indent=2))\n",
    "print(f\"Training history saved to {history_path.resolve()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7676289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to G:\\UWA Master Of Information Technology\\Semester 3 2025\\CITS4012 Natural Language Processing\\Project\\CITS4012-Group-Project\\modelC_test_predictions.json\n"
     ]
    }
   ],
   "source": [
    "sample_ids, prediction_ids = predict_labels(model, test_loader, config.device)\n",
    "prediction_labels = [ID_TO_LABEL[idx] for idx in prediction_ids]\n",
    "output_path = Path(\"modelC_test_predictions.json\")\n",
    "output_payload = [{\"id\": sid, \"prediction\": label} for sid, label in zip(sample_ids, prediction_labels)]\n",
    "output_path.write_text(json.dumps(output_payload, indent=2))\n",
    "print(f\"Test predictions saved to {output_path.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
