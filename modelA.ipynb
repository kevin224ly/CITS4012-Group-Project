{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOsq3No1kRC-"
      },
      "source": [
        "# 2025 CITS4012 Individual Assignment\n",
        "*Make sure you change the file name with your student id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THS1ppSwkgz4"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgj4JarkjAY"
      },
      "source": [
        "# 1.Dataset Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vF0lwUqfkj6p",
        "outputId": "6cb9351b-fa9d-4b2f-c985-0cc058de34e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, Model\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 0) 基础安装 & 导入\n",
        "# =========================================================\n",
        "!pip -q install pydrive2 gensim\n",
        "\n",
        "import os, re, json, unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n",
        "\n",
        "# =========================================================\n",
        "# 1) 下载数据 (换成你自己的 Google Drive 文件 ID)\n",
        "# =========================================================\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth as colab_auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "TRAIN_ID = \"1YEOo5vd8DXXUCf1FXCR1D3PxWR9XxQKv\"\n",
        "VAL_ID   = \"15FEgtzzTVDMQcNVMgwIwqoAJeF9vmtrX\"\n",
        "TEST_ID  = \"179nwaOvdkZ3ogsBaTSJvpZEIjq20uiG-\"\n",
        "\n",
        "colab_auth.authenticate_user()\n",
        "_gauth = GoogleAuth()\n",
        "_gauth.credentials = GoogleCredentials.get_application_default()\n",
        "_drive = GoogleDrive(_gauth)\n",
        "\n",
        "def gdrive_download(file_id: str, dest_path: str):\n",
        "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "    f = _drive.CreateFile({'id': file_id})\n",
        "    f.GetContentFile(dest_path)\n",
        "    print(\"Downloaded:\", dest_path)\n",
        "    return dest_path\n",
        "\n",
        "DATA_DIR = \"./data\"\n",
        "TRAIN_JSON = gdrive_download(TRAIN_ID, os.path.join(DATA_DIR, \"train.json\"))\n",
        "VAL_JSON   = gdrive_download(VAL_ID,   os.path.join(DATA_DIR, \"validation.json\"))\n",
        "TEST_JSON  = gdrive_download(TEST_ID,  os.path.join(DATA_DIR, \"test.json\"))\n",
        "\n",
        "# =========================================================\n",
        "# 2) 加载 & 简单清洗\n",
        "# =========================================================\n",
        "def load_json_as_df(path: str) -> pd.DataFrame:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def normalize(s: str, to_lower=True):\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    # 去掉多余空格\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    # 去掉非字母数字和常见标点\n",
        "    s = re.sub(r\"[^a-zA-Z0-9.,!?;:'\\\"()\\- ]\", \"\", s)\n",
        "    return s.lower() if to_lower else s\n",
        "\n",
        "train_df = load_json_as_df(TRAIN_JSON)\n",
        "val_df   = load_json_as_df(VAL_JSON)\n",
        "test_df  = load_json_as_df(TEST_JSON)\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df[\"premise_clean\"]    = df[\"premise\"].apply(normalize)\n",
        "    df[\"hypothesis_clean\"] = df[\"hypothesis\"].apply(normalize)\n",
        "\n",
        "print(train_df.head())\n",
        "\n",
        "# =========================================================\n",
        "# 3) 构建词表 & 编码\n",
        "# =========================================================\n",
        "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
        "def tokenize(s): return s.split()\n",
        "\n",
        "counter = Counter()\n",
        "for col in [\"premise_clean\",\"hypothesis_clean\"]:\n",
        "    for s in train_df[col]:\n",
        "        counter.update(tokenize(s))\n",
        "\n",
        "vocab = [PAD, UNK] + [w for w,c in counter.items() if c >= 2]\n",
        "word2id = {w:i for i,w in enumerate(vocab)}\n",
        "pad_id, unk_id = word2id[PAD], word2id[UNK]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def encode(s, max_len):\n",
        "    ids = [word2id.get(t, unk_id) for t in tokenize(s)]\n",
        "    return ids[:max_len]\n",
        "\n",
        "lens = [len(tokenize(s)) for s in train_df[\"premise_clean\"]] + \\\n",
        "       [len(tokenize(s)) for s in train_df[\"hypothesis_clean\"]]\n",
        "max_len = min(64, int(np.percentile(lens, 90)))\n",
        "print(\"Vocab:\", vocab_size, \"Max len:\", max_len)\n",
        "\n",
        "label2id = {\"entails\":0, \"neutral\":1}\n",
        "def build_inputs(df):\n",
        "    pre, hyp, y = [], [], []\n",
        "    for _,r in df.iterrows():\n",
        "        pre.append(encode(r[\"premise_clean\"], max_len))\n",
        "        hyp.append(encode(r[\"hypothesis_clean\"], max_len))\n",
        "        y.append(label2id[r[\"label\"]])\n",
        "    pre = pad_sequences(pre, maxlen=max_len, padding=\"post\", value=pad_id)\n",
        "    hyp = pad_sequences(hyp, maxlen=max_len, padding=\"post\", value=pad_id)\n",
        "    return pre,hyp,np.array(y)\n",
        "\n",
        "Xtr_p, Xtr_h, y_tr = build_inputs(train_df)\n",
        "Xv_p,  Xv_h,  y_v  = build_inputs(val_df)\n",
        "Xt_p,  Xt_h,  y_te = build_inputs(test_df)\n",
        "\n",
        "# =========================================================\n",
        "# 4) 训练 Word2Vec 并构建 embedding matrix\n",
        "# =========================================================\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "W2V_DIM = 200\n",
        "sentences = []\n",
        "for col in [\"premise_clean\", \"hypothesis_clean\"]:\n",
        "    for s in train_df[col].tolist():\n",
        "        toks = tokenize(s)\n",
        "        if toks:\n",
        "            sentences.append(toks)\n",
        "\n",
        "print(f\"Training Word2Vec on {len(sentences)} sentences ...\")\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=W2V_DIM,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=4,\n",
        "    sg=1,        # skip-gram\n",
        "    epochs=10\n",
        ")\n",
        "wv = w2v_model.wv\n",
        "\n",
        "def build_embedding_matrix(word2id, wv, dim):\n",
        "    vocab_size = len(word2id)\n",
        "    emb_mat = np.random.normal(scale=0.02, size=(vocab_size, dim)).astype(np.float32)\n",
        "    emb_mat[pad_id] = 0.0\n",
        "    hit = 0\n",
        "    for w,i in word2id.items():\n",
        "        if w in (PAD, UNK):\n",
        "            continue\n",
        "        if w in wv:\n",
        "            emb_mat[i] = wv[w]\n",
        "            hit += 1\n",
        "    print(f\"W2V coverage: {hit}/{vocab_size} = {hit/vocab_size:.2%}\")\n",
        "    return emb_mat\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(word2id, wv, W2V_DIM)\n",
        "\n",
        "# =========================================================\n",
        "# 5) 定义 Model A：BiLSTM + Cross-Attention + Pooling + MLP\n",
        "# =========================================================\n",
        "class BiAffineCrossAttention(layers.Layer):\n",
        "    def __init__(self, hidden_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_dim = hidden_dim\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(\n",
        "            name=\"bilinear_W\",\n",
        "            shape=(self.hidden_dim, self.hidden_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        Hp, Hh = inputs\n",
        "        HpW = tf.einsum(\"bij,jk->bik\", Hp, self.W)\n",
        "        S = tf.einsum(\"bid,bjd->bij\", HpW, Hh)\n",
        "        Ap = tf.nn.softmax(S, axis=-1)\n",
        "        Ah = tf.nn.softmax(tf.transpose(S, (0, 2, 1)), axis=-1)\n",
        "        Cp = tf.einsum(\"bij,bjd->bid\", Ap, Hh)\n",
        "        Ch = tf.einsum(\"bij,bjd->bid\", Ah, Hp)\n",
        "        return Cp, Ch\n",
        "\n",
        "def build_model_A_with_w2v(embedding_matrix, max_len, lstm_units=128, dropout=0.3, lr=2e-3, emb_trainable=True):\n",
        "    vocab_size, emb_dim = embedding_matrix.shape\n",
        "    inp_p = layers.Input(shape=(max_len,), name=\"premise_ids\")\n",
        "    inp_h = layers.Input(shape=(max_len,), name=\"hypothesis_ids\")\n",
        "\n",
        "    emb = layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=emb_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=emb_trainable,\n",
        "        mask_zero=False,\n",
        "        name=\"tok_emb_w2v\"\n",
        "    )\n",
        "    Ep, Eh = emb(inp_p), emb(inp_h)\n",
        "\n",
        "    Hp = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True), name=\"bilstm_p\")(Ep)\n",
        "    Hh = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True), name=\"bilstm_h\")(Eh)\n",
        "    d = int(Hp.shape[-1])\n",
        "\n",
        "    Cp, Ch = BiAffineCrossAttention(d, name=\"cross_attn\")([Hp, Hh])\n",
        "\n",
        "    diff_p = layers.Lambda(lambda x: tf.abs(x))(layers.Subtract()([Hp, Cp]))\n",
        "    prod_p = layers.Multiply()([Hp, Cp])\n",
        "    Ip = layers.Concatenate(name=\"inter_p\")([Hp, Cp, diff_p, prod_p])\n",
        "\n",
        "    diff_h = layers.Lambda(lambda x: tf.abs(x))(layers.Subtract()([Hh, Ch]))\n",
        "    prod_h = layers.Multiply()([Hh, Ch])\n",
        "    Ih = layers.Concatenate(name=\"inter_h\")([Hh, Ch, diff_h, prod_h])\n",
        "\n",
        "    vp = layers.Concatenate()([layers.GlobalMaxPooling1D()(Ip), layers.GlobalAveragePooling1D()(Ip)])\n",
        "    vh = layers.Concatenate()([layers.GlobalMaxPooling1D()(Ih), layers.GlobalAveragePooling1D()(Ih)])\n",
        "    v  = layers.Concatenate(name=\"pair_repr\")([vp, vh])\n",
        "\n",
        "    v  = layers.Dense(256, activation=\"relu\")(v)\n",
        "    v  = layers.Dropout(dropout)(v)\n",
        "    out = layers.Dense(2, activation=\"softmax\", name=\"logits\")(v)\n",
        "\n",
        "    model = Model([inp_p, inp_h], out, name=\"BiLSTM_CrossAttn_W2V\")\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "modelA = build_model_A_with_w2v(\n",
        "    embedding_matrix=embedding_matrix,\n",
        "    max_len=max_len,\n",
        "    lstm_units=128,\n",
        "    dropout=0.3,\n",
        "    lr=2e-3,\n",
        "    emb_trainable=True\n",
        ")\n",
        "modelA.summary()\n",
        "\n",
        "# =========================================================\n",
        "# 6) 训练 & 评估\n",
        "# =========================================================\n",
        "history = modelA.fit(\n",
        "    [Xtr_p, Xtr_h], y_tr,\n",
        "    validation_data=([Xv_p, Xv_h], y_v),\n",
        "    epochs=5, batch_size=64\n",
        ")\n",
        "\n",
        "val_pred = np.argmax(modelA.predict([Xv_p,Xv_h]), axis=1)\n",
        "print(\"Val acc:\", accuracy_score(y_v, val_pred))\n",
        "\n",
        "test_pred = np.argmax(modelA.predict([Xt_p,Xt_h]), axis=1)\n",
        "print(\"Test acc:\", accuracy_score(y_te, test_pred))\n",
        "print(\"\\nTest report:\\n\", classification_report(y_te, test_pred, target_names=[\"entails\",\"neutral\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDxZSK20klWr"
      },
      "source": [
        "# 2.Word Embedding Construction\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fleoeIBDkr5i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP-wrwk5ksfQ"
      },
      "source": [
        "# 3.Visualization\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgINzbHEkzbu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZJlo-z8k0ai"
      },
      "source": [
        "# 4.RNN-based Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRXewgTnk6sB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GU_rQZJk87q"
      },
      "source": [
        "# 5.Performance Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L11vnP-lk_ZH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvj4_3Qak_x7"
      },
      "source": [
        "# 6.Interactive Inference Colab Form\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMJN1Dx1lES9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
